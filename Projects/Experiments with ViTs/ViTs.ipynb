{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing required modules and classes\n",
        "from datasets import load_dataset\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "from torchvision.transforms import Compose, Normalize, Resize, ToTensor, RandomHorizontalFlip\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Loading the dataset and splitting it into training and test sets\n",
        "train_ds = load_dataset('ceyda/fashion-products-small')\n",
        "train_ds = train_ds['train'].train_test_split(test_size=0.15)\n",
        "\n",
        "# Separating the training and test data\n",
        "train_data = train_ds['train']\n",
        "test_data = train_ds['test']\n",
        "\n",
        "# Extracting labels and creating mappings from ID to label and vice versa\n",
        "label = list(set(train_data['masterCategory']))\n",
        "id2label = {id: label for id, label in enumerate(label)}\n",
        "label2id = {label: id for id, label in id2label.items()}\n",
        "\n",
        "# Initialize the Vision Transformer (ViT) image processor with a pretrained model\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Extracting the mean, standard deviation, and size for image preprocessing\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "# Defining image transformation for training and validation/test sets\n",
        "_train_transforms = Compose([\n",
        "    Resize((size, size)),\n",
        "    RandomHorizontalFlip(),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=image_mean, std=image_std),\n",
        "])\n",
        "\n",
        "_val_transforms = Compose([\n",
        "    Resize((size, size)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=image_mean, std=image_std),\n",
        "])\n",
        "\n",
        "# Functions to apply the transformations to the dataset examples\n",
        "def train_transforms(examples):\n",
        "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "def val_transforms(examples):\n",
        "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "# Applying the transformations to the datasets\n",
        "train_data.set_transform(train_transforms)\n",
        "test_data.set_transform(val_transforms)\n",
        "\n",
        "# Custom collate function for DataLoader to process batches\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([label2id[example[\"masterCategory\"]] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "# Creating DataLoaders for training and testing\n",
        "train_dataloader = DataLoader(train_data, collate_fn=collate_fn, batch_size=4)\n",
        "test_dataloader = DataLoader(test_data, collate_fn=collate_fn, batch_size=4)\n",
        "\n",
        "# Loading a pretrained ViT model for image classification\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Setting training arguments\n",
        "args = TrainingArguments(\n",
        "    \"Fashion-Product-Images\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    logging_dir='logs',\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Define a function to compute metrics (e.g., accuracy) for evaluation - Function not provided in your code\n",
        "\n",
        "# Initializing the Trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,  # This function needs to be defined\n",
        "    tokenizer=processor,\n",
        ")\n",
        "\n",
        "# Starting the training process\n",
        "trainer.train()\n",
        "\n",
        "# Evaluating the model on the test dataset\n",
        "outputs = trainer.predict(test_data)\n",
        "print(outputs.metrics)\n"
      ],
      "metadata": {
        "id": "z1isXnqyDtDp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}